# ğŸš€ LLM API Testing Framework

---

RAG stands forÂ Retrieval-Augmented Generation, an AI framework that enhances a Large Language Model's (LLM) capabilities by combining it with an external information retrieval system

---

This project is a **Streamlit-powered dashboard + CLI framework** for automated API testing using OpenAPI specifications.  
It generates test cases (headers, parameters, body validation, security checks, boundary tests) and validates responses with both **rule-based checks** and **LLM-based validation**.

---

## âœ¨ Features
- ğŸ“‘ Parse OpenAPI spec and auto-generate test cases  
- ğŸ› ï¸ API test execution (CLI or Streamlit UI)  
- âœ… Validations for:
  - Headers
  - Parameters
  - Body schema (required, types, enums, boundaries, combinations)
  - Security checks (e.g., `x-session-token`)  
- ğŸ¤– LLM Validation (via [Perplexity API](https://docs.perplexity.ai))  
- ğŸ“Š CSV reports with pass/fail results  
- ğŸ’¬ Knowledge Base Chat (ask questions against docs/PDFs)  

---

## ğŸ—‚ Project Structure
LLM-API-Testing/

â”‚â”€â”€ app.py # Streamlit Dashboard

â”‚â”€â”€ main.py # CLI Test Runner

â”‚â”€â”€ config.py # Configurations (âš ï¸ don't commit secrets!)

â”‚â”€â”€ requirements.txt # Python dependencies
â”‚â”€â”€ README.md # Project documentation
â”œâ”€â”€ openapi/ # OpenAPI loaders & helpers
â”œâ”€â”€ tests/ # Test generators & executors
â”œâ”€â”€ knowledgebase/ # Knowledge base loader

â”œâ”€â”€ utils/ # Utility functions (file/text handling)
â”œâ”€â”€ reports/ # Test execution CSV reports
â””â”€â”€ testdata/ # API spec YAML/JSON files



---

## âš™ï¸ Installation
Clone the repo and install dependencies:

```bash
   pip install -r requirements.txt
```

Run with Streamlit Dashboard
```bash
  streamlit run app.py

```
Run with CLI

```bash
  python main.py --run-tests


```